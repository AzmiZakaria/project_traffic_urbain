from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import json
import os
from kafka import KafkaConsumer
from hdfs import InsecureClient

# --- CONFIGURATION INTERNE DOCKER ---
KAFKA_HOST = 'kafka:9092'  # Port interne
NAMENODE_URL = 'http://namenode:9870'
TOPIC = 'traffic-events'
HDFS_PATH = '/data/raw/traffic'

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

def consume_and_store_hdfs():
    print("ðŸš€ DÃ©marrage Job Ingestion Kafka -> HDFS")
    
    # 1. Connexion HDFS
    # Note: Dans ton rÃ©seau Docker, le namenode est accessible via son hostname
    try:
        client = InsecureClient(NAMENODE_URL, user='root')
        print("âœ… Connexion HDFS OK")
    except Exception as e:
        print(f"âŒ Erreur HDFS: {e}")
        raise

    # 2. Consumer Kafka
    # On utilise un group_id pour reprendre la lecture lÃ  oÃ¹ on s'est arrÃªtÃ©
    consumer = KafkaConsumer(
        TOPIC,
        bootstrap_servers=KAFKA_HOST,
        group_id='airflow-hdfs-loader',
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        consumer_timeout_ms=5000, # ArrÃªte de lire aprÃ¨s 5s de silence
        value_deserializer=lambda x: json.loads(x.decode('utf-8'))
    )

    messages = []
    print("ðŸ“¥ Lecture des messages Kafka...")
    
    for message in consumer:
        messages.append(message.value)
        if len(messages) >= 1000: # Batch max
            break
            
    if not messages:
        print("âš ï¸ Aucun nouveau message.")
        return

    # 3. Ecriture dans HDFS
    filename = f"traffic_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    hdfs_file_path = f"{HDFS_PATH}/{filename}"
    
    # Convertir en NDJSON (Newline Delimited JSON) pour Spark
    content = "\n".join([json.dumps(msg) for msg in messages])
    
    with client.write(hdfs_file_path, encoding='utf-8') as writer:
        writer.write(content)
        
    print(f"ðŸ’¾ SauvegardÃ© : {hdfs_file_path} ({len(messages)} events)")

with DAG(
    'traffic_ingestion_kafka_hdfs',
    default_args=default_args,
    description='Ingestion Kafka vers HDFS',
    schedule_interval=timedelta(minutes=2), # Lance toutes les 2 minutes
    catchup=False,
) as dag:

    ingest_task = PythonOperator(
        task_id='consume_kafka_write_hdfs',
        python_callable=consume_and_store_hdfs,
    )
    # 2. TÃ¢che de Processing (Spark -> Postgres)
    process_task = BashOperator(
        task_id='trigger_spark_job',
        bash_command="""
        docker exec spark-master /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --jars /opt/jobs/postgresql-42.7.3.jar \
          /opt/jobs/process_traffic.py
        """
    )

    # 3. Ordonnancement
    # On dit : "Fais l'ingestion D'ABORD, et SI c'est bon, lance Spark"
    ingest_task >> process_task